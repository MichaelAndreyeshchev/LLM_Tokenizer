# -*- coding: utf-8 -*-
"""Byte_Pair_Encoding_LLM_Tokenizer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JFd8ejpmeptiY_ALQO6NQWjhPgHUIegH
"""

!pip install datasets
!pip install tiktoken
!pip install gutenbergpy

"""# Importing Necessary Libraries"""

import re
import importlib
import tiktoken
from torch.utils.data import Dataset, DataLoader
from datasets import load_dataset
import requests

"""# Getting Our Corpus

The notebook retrieves the full text of Moby-Dick from Project Gutenberg. The text is stored in the variable **corpus** for tokenization.
"""

url = "https://www.gutenberg.org/files/2701/2701-0.txt" # The  Moby Dick book will be used for this
response = requests.get(url)

corpus = response.text

"""# Instantiating the Byte Pair Encoding Tokenizer"""

tokenizer = tiktoken.get_encoding("gpt2") # THIS IS THE BPE TOKENIZER!!!

"""# Creating the BPE Tokenizer class for ease of use

This is the Byte Pair Tokenizer code which uses OpenAI's tiktoken library.
"""

class BytePairTokenizer:
  def __init__(self, tokenizer):
    self.tokenizer = tokenizer

  def encode(self, text):
    return self.tokenizer.encode(text, allowed_special={"<|endoftext|>"})

  def decode(self, tokens):
    return self.tokenizer.decode(tokens)

tokenizer = BytePairTokenizer(tokenizer)
text = """Can you please, please give me your name so I can see who you actually are."""
token_ids = tokenizer.encode(text)
decoded_text = tokenizer.decode(token_ids)
print(token_ids)
print(decoded_text)