{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d12_okXi4z3v",
        "outputId": "c9821f30-02b8-4598-acd2-46e7132871cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.0.2-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.16.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Downloading datasets-3.0.2-py3-none-any.whl (472 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.7/472.7 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-3.0.2 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.8.0\n",
            "Collecting gutenbergpy\n",
            "  Downloading gutenbergpy-0.3.5-py3-none-any.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.10/dist-packages (from gutenbergpy) (1.0.0)\n",
            "Collecting httpsproxy-urllib2 (from gutenbergpy)\n",
            "  Downloading httpsproxy_urllib2-1.0.tar.gz (28 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from gutenbergpy) (4.9.4)\n",
            "Collecting pymongo (from gutenbergpy)\n",
            "  Downloading pymongo-4.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from gutenbergpy) (75.1.0)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from gutenbergpy) (5.2.0)\n",
            "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo->gutenbergpy)\n",
            "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Downloading gutenbergpy-0.3.5-py3-none-any.whl (22 kB)\n",
            "Downloading pymongo-4.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: httpsproxy-urllib2\n",
            "  Building wheel for httpsproxy-urllib2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for httpsproxy-urllib2: filename=httpsproxy_urllib2-1.0-py3-none-any.whl size=29250 sha256=0ae0fe2bf147d0a30f8acf4a87f66c9cfdcbe07727233f8e68d988e61232d4c1\n",
            "  Stored in directory: /root/.cache/pip/wheels/1b/fa/c3/4c14e72101070c40b56c2bfb4617e510e68f121e4f736a5d2a\n",
            "Successfully built httpsproxy-urllib2\n",
            "Installing collected packages: httpsproxy-urllib2, dnspython, pymongo, gutenbergpy\n",
            "Successfully installed dnspython-2.7.0 gutenbergpy-0.3.5 httpsproxy-urllib2-1.0 pymongo-4.10.1\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "!pip install tiktoken\n",
        "!pip install gutenbergpy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Necessary Libraries"
      ],
      "metadata": {
        "id": "henAn7vF644w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1oZXOoY5117"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import importlib\n",
        "import tiktoken\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datasets import load_dataset\n",
        "import requests"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting Our Corpus"
      ],
      "metadata": {
        "id": "mFpK4LYe68Fv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The notebook retrieves the full text of Moby-Dick from Project Gutenberg. The text is stored in the variable **corpus** for tokenization."
      ],
      "metadata": {
        "id": "yuyPKiyyYOuY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://www.gutenberg.org/files/2701/2701-0.txt\" # The  Moby Dick book will be used for this\n",
        "response = requests.get(url)\n",
        "\n",
        "corpus = response.text"
      ],
      "metadata": {
        "id": "I90LNiJT6TXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Basic Tokenizer from Corpus"
      ],
      "metadata": {
        "id": "V45Jh-0V7GQT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This initial version of the tokenizer processes the corpus by:\n",
        "* Splitting the text using regular expressions to capture common punctuation and whitespace as tokens.\n",
        "* Constructing a vocabulary as a sorted, unique list of words and punctuation marks from the corpus.\n",
        "* Mapping each word to a unique ID in **vocab_to_id** for encoding and in **id_to_vocab** for decoding.\n",
        "\n",
        "The tokenizer provides two main functions:\n",
        "1. **encode(text)**: Converts a given text into a list of token IDs based on the vocabulary.\n",
        "2. **decode(tokens)**: Reconstructs the original text from token IDs, reattaching punctuation correctly."
      ],
      "metadata": {
        "id": "pFCcQw4FYm1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LLMBasicTokenizerV1:\n",
        "  def __init__(self, corpus):\n",
        "    processed_corpus = re.split(r'([,.;:?!_\"()\\']--|\\s)', corpus)\n",
        "    processed_corpus = [token.strip() for token in processed_corpus if token.strip()]\n",
        "\n",
        "    vocab = sorted(set(processed_corpus))\n",
        "    self.vocab_to_id = {word:i for i, word in enumerate(vocab)}\n",
        "    self.id_to_vocab = {i:word for i, word in enumerate(vocab)}\n",
        "\n",
        "  def encode(self, text):\n",
        "    split_text = re.split(r'([,.;:?!_\"()\\']--|\\s)', text)\n",
        "    return [self.vocab_to_id[token.strip()] for token in split_text if token.strip() in self.vocab_to_id]\n",
        "\n",
        "  def decode(self, tokens):\n",
        "    joined_tokens = \" \".join([self.id_to_vocab[token] for token in tokens])\n",
        "    return re.sub(r'\\s+([,.;:?!_\"()\\'])', r'\\1', joined_tokens)\n"
      ],
      "metadata": {
        "id": "BSDZ0O837DmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = LLMBasicTokenizerV1(corpus)\n",
        "text = \"\"\"Can you please, please give me your name so I can see who you actually are.\"\"\"\n",
        "token_ids = tokenizer.encode(text)\n",
        "decoded_text = tokenizer.decode(token_ids)\n",
        "print(token_ids)\n",
        "print(decoded_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itE_BdXy7MMT",
        "outputId": "064277c7-43da-4146-d832-54701b902748"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1046, 32687, 22807, 22805, 14878, 19774, 32705, 20703, 26968, 2365, 8371, 25732, 32086, 32687, 5284, 6145]\n",
            "Can you please, please give me your name so I can see who you actually are.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a More Advanced Tokenizer"
      ],
      "metadata": {
        "id": "dLy1wXXzCV9r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* This version extends LLMBasicTokenizerV1 by introducing special tokens, <|endoftext|> and <|unk|>, to handle text boundaries and unknown words.\n",
        "* This tokenizer allows better handling of unknown or out-of-vocabulary tokens by introducing an unknown token placeholder.\n",
        "* Like LLMBasicTokenizerV1, it provides the encode and decode methods, enhancing flexibility for more robust tokenization."
      ],
      "metadata": {
        "id": "zmDxk6bPZCF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LLMBasicTokenizerV2:\n",
        "  def __init__(self, corpus):\n",
        "    processed_corpus = re.split(r'([,.;:?!_\"()\\']--|\\s)', corpus)\n",
        "    processed_corpus = [token.strip() for token in processed_corpus if token.strip()]\n",
        "\n",
        "    vocab = sorted(set(processed_corpus))\n",
        "    vocab.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
        "    self.vocab_to_id = {word:i for i, word in enumerate(vocab)}\n",
        "    self.id_to_vocab = {i:word for i, word in enumerate(vocab)}\n",
        "\n",
        "  def encode(self, text):\n",
        "    split_text = re.split(r'([,.;:?!_\"()\\']--|\\s)', text)\n",
        "    return [self.vocab_to_id[token.strip()] for token in split_text if token.strip() in self.vocab_to_id]\n",
        "\n",
        "  def decode(self, tokens):\n",
        "    joined_tokens = \" \".join([self.id_to_vocab[token] for token in tokens])\n",
        "    return re.sub(r'\\s+([,.;:?!_\"()\\'])', r'\\1', joined_tokens)\n"
      ],
      "metadata": {
        "id": "aX9nnVL4Cd-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = LLMBasicTokenizerV2(corpus)\n",
        "text = \"\"\"Timothy, can you please, please give me your name so I can see who you actually are.\"\"\"\n",
        "token_ids = tokenizer.encode(text)\n",
        "decoded_text = tokenizer.decode(token_ids)\n",
        "print(token_ids)\n",
        "print(decoded_text)"
      ],
      "metadata": {
        "id": "8c3tA_RjC7bB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eba5ad9e-5c2f-4cf1-9266-a9a00e468795"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8371, 32687, 22807, 22805, 14878, 19774, 32705, 20703, 26968, 2365, 8371, 25732, 32086, 32687, 5284, 6145]\n",
            "can you please, please give me your name so I can see who you actually are.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}