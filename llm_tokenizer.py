# -*- coding: utf-8 -*-
"""LLM_Tokenizer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vwYbpBDGK2ZOcpAj9v0zT2poLtw1Nm-d
"""

!pip install datasets
!pip install tiktoken
!pip install gutenbergpy

"""# Importing Necessary Libraries"""

import re
import importlib
import tiktoken
from torch.utils.data import Dataset, DataLoader
from datasets import load_dataset
import requests

"""# Getting Our Corpus

The notebook retrieves the full text of Moby-Dick from Project Gutenberg. The text is stored in the variable **corpus** for tokenization.
"""

url = "https://www.gutenberg.org/files/2701/2701-0.txt" # The  Moby Dick book will be used for this
response = requests.get(url)

corpus = response.text

"""# Creating Basic Tokenizer from Corpus

This initial version of the tokenizer processes the corpus by:
* Splitting the text using regular expressions to capture common punctuation and whitespace as tokens.
* Constructing a vocabulary as a sorted, unique list of words and punctuation marks from the corpus.
* Mapping each word to a unique ID in **vocab_to_id** for encoding and in **id_to_vocab** for decoding.

The tokenizer provides two main functions:
1. **encode(text)**: Converts a given text into a list of token IDs based on the vocabulary.
2. **decode(tokens)**: Reconstructs the original text from token IDs, reattaching punctuation correctly.
"""

class LLMBasicTokenizerV1:
  def __init__(self, corpus):
    processed_corpus = re.split(r'([,.;:?!_"()\']--|\s)', corpus)
    processed_corpus = [token.strip() for token in processed_corpus if token.strip()]

    vocab = sorted(set(processed_corpus))
    self.vocab_to_id = {word:i for i, word in enumerate(vocab)}
    self.id_to_vocab = {i:word for i, word in enumerate(vocab)}

  def encode(self, text):
    split_text = re.split(r'([,.;:?!_"()\']--|\s)', text)
    return [self.vocab_to_id[token.strip()] for token in split_text if token.strip() in self.vocab_to_id]

  def decode(self, tokens):
    joined_tokens = " ".join([self.id_to_vocab[token] for token in tokens])
    return re.sub(r'\s+([,.;:?!_"()\'])', r'\1', joined_tokens)

tokenizer = LLMBasicTokenizerV1(corpus)
text = """Can you please, please give me your name so I can see who you actually are."""
token_ids = tokenizer.encode(text)
decoded_text = tokenizer.decode(token_ids)
print(token_ids)
print(decoded_text)

"""# Creating a More Advanced Tokenizer

* This version extends LLMBasicTokenizerV1 by introducing special tokens, <|endoftext|> and <|unk|>, to handle text boundaries and unknown words.
* This tokenizer allows better handling of unknown or out-of-vocabulary tokens by introducing an unknown token placeholder.
* Like LLMBasicTokenizerV1, it provides the encode and decode methods, enhancing flexibility for more robust tokenization.
"""

class LLMBasicTokenizerV2:
  def __init__(self, corpus):
    processed_corpus = re.split(r'([,.;:?!_"()\']--|\s)', corpus)
    processed_corpus = [token.strip() for token in processed_corpus if token.strip()]

    vocab = sorted(set(processed_corpus))
    vocab.extend(["<|endoftext|>", "<|unk|>"])
    self.vocab_to_id = {word:i for i, word in enumerate(vocab)}
    self.id_to_vocab = {i:word for i, word in enumerate(vocab)}

  def encode(self, text):
    split_text = re.split(r'([,.;:?!_"()\']--|\s)', text)
    return [self.vocab_to_id[token.strip()] for token in split_text if token.strip() in self.vocab_to_id]

  def decode(self, tokens):
    joined_tokens = " ".join([self.id_to_vocab[token] for token in tokens])
    return re.sub(r'\s+([,.;:?!_"()\'])', r'\1', joined_tokens)

tokenizer = LLMBasicTokenizerV2(corpus)
text = """Timothy, can you please, please give me your name so I can see who you actually are."""
token_ids = tokenizer.encode(text)
decoded_text = tokenizer.decode(token_ids)
print(token_ids)
print(decoded_text)